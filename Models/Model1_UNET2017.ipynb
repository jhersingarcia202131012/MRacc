{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from Utils.DataConsistency import dataconsitency"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T00:05:13.852657400Z",
     "start_time": "2024-03-18T00:05:12.325634400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T00:05:13.869612300Z",
     "start_time": "2024-03-18T00:05:13.852657400Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.deconv(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T00:05:13.881579400Z",
     "start_time": "2024-03-18T00:05:13.863653700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\" CNN Encoder \"\"\"\n",
    "        ## Encoder 1\n",
    "        self.e1 = nn.Sequential(\n",
    "            ConvBlock(1, 64),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "        \n",
    "        self.e2 = nn.Sequential(\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.e3 = nn.Sequential(\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.e4 = nn.Sequential(\n",
    "            ConvBlock(256, 512),\n",
    "            ConvBlock(512, 512)\n",
    "        )       \n",
    "        \n",
    "        self.maxpool1 = torch.nn.MaxPool2d(2)\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(2)\n",
    "        self.maxpool3 = torch.nn.MaxPool2d(2)\n",
    "        self.maxpool4 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "        \"\"\" CNN Decoder \"\"\"\n",
    "        ## Decoder 1\n",
    "        self.d1 = DeconvBlock(512, 512)\n",
    "        self.c1 = nn.Sequential(\n",
    "            ConvBlock(512+512, 512),\n",
    "            ConvBlock(512, 512)\n",
    "        )\n",
    "\n",
    "        ## Decoder 2\n",
    "        self.d2 = DeconvBlock(512, 256)\n",
    "        self.c2 = nn.Sequential(\n",
    "            ConvBlock(256+256, 256),\n",
    "            ConvBlock(256, 256)\n",
    "        )\n",
    "\n",
    "        ## Decoder 3\n",
    "        self.d3 = DeconvBlock(256, 128)\n",
    "        self.c3 = nn.Sequential(\n",
    "            ConvBlock(128+128, 128),\n",
    "            ConvBlock(128, 128)\n",
    "        )\n",
    "\n",
    "        ## Decoder 4\n",
    "        self.d4 = DeconvBlock(128, 64)\n",
    "        self.c4 = nn.Sequential(\n",
    "            ConvBlock(64+64, 64),\n",
    "            ConvBlock(64, 64)\n",
    "        )\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        self.output = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, X ):\n",
    "    \n",
    "        \"\"\" CNN Encoder \"\"\"\n",
    "        ## Encoder 1\n",
    "        e1 = self.e1(X)\n",
    "        print('Encoder1',e1.shape)\n",
    "        e1m = self.maxpool1(e1)\n",
    "        print('Encoder1m',e1m.shape)\n",
    "        \n",
    "        ## Encoder 2\n",
    "        e2 = self.e2(e1m)\n",
    "        print('Encoder2',e2.shape)\n",
    "        e2m = self.maxpool2(e2)\n",
    "        print('Encoder2m',e2m.shape)\n",
    "        \n",
    "        ## Encoder 3\n",
    "        e3 = self.e3(e2m)\n",
    "        print('Encoder3',e3.shape)\n",
    "        e3m = self.maxpool3(e3)\n",
    "        print('Encoder3m',e3m.shape)\n",
    "        \n",
    "        ## Encoder 4\n",
    "        e4 = self.e4(e3m)\n",
    "        print('Encoder4',e4.shape)\n",
    "        e4m = self.maxpool4(e4)\n",
    "        print('Encoder4m',e4m.shape)\n",
    "        \n",
    "        \"\"\" CNN Decoder \"\"\"\n",
    "        print('Decoder')\n",
    "        \n",
    "        ## Decoder 1\n",
    "        d1 = self.d1(e4m)\n",
    "        print('Decoder1',d1.shape)\n",
    "        print('Encoder4',e4.shape)\n",
    "        c1d = torch.cat([d1, e4], dim=1)\n",
    "        print('Concatenacion1',c1d.shape)\n",
    "        c1 = self.c1(c1d)\n",
    "        print('Concatenacion1 + Reduccion',c1.shape)\n",
    "        \n",
    "        ## Decoder 2\n",
    "        d2 = self.d2(c1)\n",
    "        print('Decoder2',d2.shape)\n",
    "        c2d = torch.cat([d2, e3], dim=1)\n",
    "        print('Concatenacion2',c2d.shape)\n",
    "        c2 = self.c2(c2d)\n",
    "        print('Concatenacion2 + Reduccion',c2.shape)\n",
    "        \n",
    "        \n",
    "        ## Decoder 3\n",
    "        d3 = self.d3(c2)\n",
    "        print('Decoder3',d3.shape)\n",
    "        c3d = torch.cat([d3, e2], dim=1)\n",
    "        print('Concatenacion3',c3d.shape)\n",
    "        c3 = self.c3(c3d)\n",
    "        print('Concatenacion3 + Reduccion',c3.shape)\n",
    "        \n",
    "        ## Decoder 4\n",
    "        d4 = self.d4(c3)\n",
    "        print('Decoder4',d4.shape)\n",
    "        c4d = torch.cat([d4, e1], dim=1)\n",
    "        print('Concatenacion4',c4d.shape)\n",
    "        c4 = self.c4(c4d)\n",
    "        print('Concatenacion4 + Reduccion',c4.shape)\n",
    "        \n",
    "        \n",
    "        \"\"\" Output \"\"\"\n",
    "        output1 = self.output(c4)\n",
    "        print('Output1',output1.shape)\n",
    "        output = dataconsitency(X, output1)\n",
    "        print('Output',output.shape)\n",
    "   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T00:05:13.895543Z",
     "start_time": "2024-03-18T00:05:13.886567100Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder1 torch.Size([8, 64, 320, 320])\n",
      "Encoder1m torch.Size([8, 64, 160, 160])\n",
      "Encoder2 torch.Size([8, 128, 160, 160])\n",
      "Encoder2m torch.Size([8, 128, 80, 80])\n",
      "Encoder3 torch.Size([8, 256, 80, 80])\n",
      "Encoder3m torch.Size([8, 256, 40, 40])\n",
      "Encoder4 torch.Size([8, 512, 40, 40])\n",
      "Encoder4m torch.Size([8, 512, 20, 20])\n",
      "Decoder\n",
      "Decoder1 torch.Size([8, 512, 40, 40])\n",
      "Encoder4 torch.Size([8, 512, 40, 40])\n",
      "Concatenacion1 torch.Size([8, 1024, 40, 40])\n",
      "Concatenacion1 + Reduccion torch.Size([8, 512, 40, 40])\n",
      "Decoder2 torch.Size([8, 256, 80, 80])\n",
      "Concatenacion2 torch.Size([8, 512, 80, 80])\n",
      "Concatenacion2 + Reduccion torch.Size([8, 256, 80, 80])\n",
      "Decoder3 torch.Size([8, 128, 160, 160])\n",
      "Concatenacion3 torch.Size([8, 256, 160, 160])\n",
      "Concatenacion3 + Reduccion torch.Size([8, 128, 160, 160])\n",
      "Decoder4 torch.Size([8, 64, 320, 320])\n",
      "Concatenacion4 torch.Size([8, 128, 320, 320])\n",
      "Concatenacion4 + Reduccion torch.Size([8, 64, 320, 320])\n",
      "Output1 torch.Size([8, 1, 320, 320])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m model4 \u001B[38;5;241m=\u001B[39m model4\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m      3\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m320\u001B[39m, \u001B[38;5;241m320\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m----> 4\u001B[0m conv \u001B[38;5;241m=\u001B[39m \u001B[43mmodel4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[5], line 132\u001B[0m, in \u001B[0;36mUNet.forward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    130\u001B[0m output1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(c4)\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput1\u001B[39m\u001B[38;5;124m'\u001B[39m,output1\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m--> 132\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mdataconsitency\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOutput\u001B[39m\u001B[38;5;124m'\u001B[39m,output\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\Utils\\DataConsistency.py:13\u001B[0m, in \u001B[0;36mdataconsitency\u001B[1;34m(lr_image, hr_image)\u001B[0m\n\u001B[0;32m     10\u001B[0m hr_kspace \u001B[38;5;241m=\u001B[39m transform_image_to_kspace(hr_image, dim\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m))\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Convert numpy mask to PyTorch tensor\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[43mgen_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr_kspace\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccel_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAF\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m mask_inv \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39mmask\n\u001B[0;32m     15\u001B[0m fourier_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(np\u001B[38;5;241m.\u001B[39mrepeat(mask\u001B[38;5;241m.\u001B[39mfloat(), lr_kspace\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(lr_kspace\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\Utils\\gen_mask_tensores.py:12\u001B[0m, in \u001B[0;36mgen_mask\u001B[1;34m(kspace, accel_factor, seed)\u001B[0m\n\u001B[0;32m      9\u001B[0m acceleration \u001B[38;5;241m=\u001B[39m accel_factor\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Create the mask\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m num_low_freqs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mround\u001B[39m(\u001B[43mnum_cols\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m() \u001B[38;5;241m*\u001B[39m center_fraction))\n\u001B[0;32m     13\u001B[0m prob \u001B[38;5;241m=\u001B[39m (num_cols \u001B[38;5;241m/\u001B[39m acceleration \u001B[38;5;241m-\u001B[39m num_low_freqs) \u001B[38;5;241m/\u001B[39m (num_cols \u001B[38;5;241m-\u001B[39m num_low_freqs)\n\u001B[0;32m     14\u001B[0m mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrand(num_cols) \u001B[38;5;241m<\u001B[39m prob\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "model4 = UNet()\n",
    "model4 = model4.cuda()\n",
    "x = torch.randn(8, 1, 320, 320).cuda()\n",
    "conv = model4(x)\n",
    "#conv.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T00:05:15.019470500Z",
     "start_time": "2024-03-18T00:05:13.893551800Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
